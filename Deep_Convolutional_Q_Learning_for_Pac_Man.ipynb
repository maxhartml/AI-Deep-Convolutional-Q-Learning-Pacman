{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxhart5000/AI-Deep-Convolutional-Q-Learning-Pacman/blob/main/Deep_Convolutional_Q_Learning_for_Pac_Man.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAiHVEoWHy_D"
      },
      "source": [
        "# Deep Convolutional Q-Learning for Pac-Man"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwdRB-ZLdrAV"
      },
      "source": [
        "### Installing Gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbnq3XpoKa_7",
        "outputId": "0b027aec-b658-468c-c7c4-c9cfe972cb82"
      },
      "outputs": [],
      "source": [
        "# install gymnasium\n",
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-wes4LZdxdd"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ho_25-9_9qnu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlYVpVdHe-i6"
      },
      "source": [
        "### Creating the architecture of the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OJkx_M_AKBuL"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "  def __init__(self, action_size, seed = 42):\n",
        "    super(Network, self).__init__()\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    self.conv1 = nn.Conv2d(3, 32, kernel_size = 8, stride = 4)\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.conv2 = nn.Conv2d(32,64, kernel_size = 4, stride = 2)\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.conv3 = nn.Conv2d(64, 64, kernel_size = 3, stride = 1)\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.conv4 = nn.Conv2d(64, 128, kernel_size = 3, stride = 1)\n",
        "    self.bn4 = nn.BatchNorm2d(128)\n",
        "    self.fc1 = nn.Linear(10 * 10 * 128, 512)\n",
        "    self.fc2 = nn.Linear(512, 256)\n",
        "    self.fc3 = nn.Linear(256, action_size)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = F.relu(self.bn1(self.conv1(state)))\n",
        "    x = F.relu(self.bn2(self.conv2(x)))\n",
        "    x = F.relu(self.bn3(self.conv3(x)))\n",
        "    x = F.relu(self.bn4(self.conv4(x)))\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    return self.fc3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWCDPF22lkwc"
      },
      "source": [
        "### Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRPCv03J2RET",
        "outputId": "d3bbc48d-525b-4f0a-8f5e-69af288ec3f7"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make('MsPacmanDeterministic-v0', full_action_space = False)\n",
        "state_shape = env.observation_space.shape\n",
        "state_size = env.observation_space.shape[0]\n",
        "number_actions = env.action_space.n\n",
        "\n",
        "print('State shape: ', state_shape)\n",
        "print('State size: ', state_size)\n",
        "print('Number of Actions: ', number_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx6IdX3ciDqH"
      },
      "source": [
        "### Initializing the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N58mgyQ6IXaT",
        "outputId": "96a962d7-2153-4a35-e71b-4e51895a087e"
      },
      "outputs": [],
      "source": [
        "# Learning Rate: Determines the step size for the optimizer when updating the network's weights.\n",
        "# A smaller learning rate can lead to more stable but slower convergence.\n",
        "learning_rate = 5e-4\n",
        "\n",
        "# Minibatch Size: The number of samples drawn from the replay buffer for each training step.\n",
        "# Larger minibatches provide more stable gradient estimates but require more memory.\n",
        "minibatch_size = 64\n",
        "\n",
        "# Discount Factor (Î³): Balances the importance of immediate and future rewards.\n",
        "# A value close to 1 (e.g., 0.99) emphasizes long-term rewards.\n",
        "discount_factor = 0.99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2bDShIEkA5V"
      },
      "source": [
        "### Preprocessing the frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0F_fdI6M0VXY"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "  frame = Image.fromarray(frame)\n",
        "  preprocess = transforms.Compose([transforms.Resize((128,128)), transforms.ToTensor()])\n",
        "  return preprocess(frame).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imMdSO-HAWra"
      },
      "source": [
        "### Implementing the DCQN class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV6-0vNkCsGg",
        "outputId": "1fc1691c-066f-4830-c044-b1210e7700c5"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "\n",
        "  def __init__(self, action_size):\n",
        "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    self.action_size = action_size\n",
        "    self.local_qnetwork = Network(action_size).to(self.device)\n",
        "    self.target_qnetwork = Network(action_size).to(self.device)\n",
        "    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate)\n",
        "    self.memory = deque(maxlen = 10_000)\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    state = preprocess_frame(state)\n",
        "    next_state = preprocess_frame(next_state)\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "    if len(self.memory) > minibatch_size:\n",
        "      experiences = random.sample(self.memory, k = minibatch_size)\n",
        "      self.learn(experiences, discount_factor)\n",
        "\n",
        "  def act(self, state, epsilon = 0.):\n",
        "    state = preprocess_frame(state).to(self.device)\n",
        "    self.local_qnetwork.eval()\n",
        "    with torch.no_grad():\n",
        "      action_values = self.local_qnetwork(state)\n",
        "    self.local_qnetwork.train()\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "  def learn(self, experiences, discount_factor):\n",
        "    states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "    states = torch.from_numpy(np.vstack(states)).float().to(self.device)\n",
        "    actions = torch.from_numpy(np.vstack(actions)).long().to(self.device)\n",
        "    rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device)\n",
        "    next_states = torch.from_numpy(np.vstack(next_states)).float().to(self.device)\n",
        "    dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(self.device)\n",
        "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    q_targets = rewards + (discount_factor * next_q_targets * (1-dones))\n",
        "    q_expected = self.local_qnetwork(states).gather(1, actions)\n",
        "    loss = F.mse_loss(q_expected, q_targets)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUg95iBpAwII"
      },
      "source": [
        "### Initializing the DCQN agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_evaUmjDU8AT"
      },
      "outputs": [],
      "source": [
        "agent = Agent(number_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK6Zt_gNmHvm"
      },
      "source": [
        "### Training the DCQN agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "WsWmZtIQU8Sk",
        "outputId": "dd857328-5ba4-43d6-bb49-df5838dac68d"
      },
      "outputs": [],
      "source": [
        "number_episodes = 2000\n",
        "maximum_number_timesteps_per_episode = 10_000\n",
        "epsilon_starting_value =  1.0\n",
        "epsilon_ending_value = 0.01\n",
        "epsilon_decay_value = 0.995\n",
        "epsilon = epsilon_starting_value\n",
        "scores_on_100_episodes = deque(maxlen = 100)\n",
        "\n",
        "for episode in range(1, number_episodes + 1):\n",
        "  state, _ = env.reset()\n",
        "  score = 0\n",
        "  for t in range(maximum_number_timesteps_per_episode):\n",
        "    action = agent.act(state, epsilon)\n",
        "    next_state, reward, done, _,  _ = env.step(action)\n",
        "    agent.step(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if done:\n",
        "      break\n",
        "  scores_on_100_episodes.append(score)\n",
        "  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
        "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = '')\n",
        "  if episode % 100 == 0:\n",
        "     print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
        "  if np.mean(scores_on_100_episodes) >= 500.0:\n",
        "     print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n",
        "     torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
        "     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb9nVvU2Okhk"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'MsPacmanDeterministic-v0')\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AI_ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
